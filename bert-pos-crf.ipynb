{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://github.com/harvardnlp/pytorch-struct/blob/master/notebooks/BertTagger.ipynb\n",
    "# !pip install -qqq torchtext wandb pytorch-transformers\n",
    "# !pip install -qqqU git+https://github.com/harvardnlp/pytorch-struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_struct import LinearChainCRF\n",
    "import torch_struct.data\n",
    "import torchtext.data as data\n",
    "from pytorch_transformers import *\n",
    "config = {\"bert\": \"bert-base-cased\", \"H\" : 768, \"dropout\": 0.2}\n",
    "\n",
    "# Comment or add your wandb\n",
    "#import wandb\n",
    "#wandb.init(project=\"pytorch-struct-tagging\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConllXDataset(data.Dataset):\n",
    "    def __init__(self, path, fields, encoding='utf-8', separator='\\t', **kwargs):\n",
    "        examples = []\n",
    "        columns = [[], []]\n",
    "        column_map = {1: 0, 3: 1}\n",
    "        with open(path, encoding=encoding) as input_file:\n",
    "            for line in input_file:\n",
    "                line = line.strip()\n",
    "                if line == '':\n",
    "                    examples.append(data.Example.fromlist(columns, fields))\n",
    "                    columns = [[], []]\n",
    "                else:\n",
    "                    for i, column in enumerate(line.split(separator)):\n",
    "                        if i in column_map:\n",
    "                            columns[column_map[i]].append(column)\n",
    "            examples.append(data.Example.fromlist(columns, fields))\n",
    "        super(ConllXDataset, self).__init__(examples, fields, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = BertModel, BertTokenizer, config[\"bert\"]\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)    \n",
    "WORD = torch_struct.data.SubTokenizedField(tokenizer)\n",
    "UD_TAG = torchtext.data.Field(init_token=\"<bos>\", eos_token=\"<eos>\", include_lengths=True)\n",
    "\n",
    "# train, val, test = torchtext.datasets.UDPOS.splits(\n",
    "#     fields=(('word', WORD), ('udtag', UD_TAG), (None, None)), \n",
    "#     filter_pred=lambda ex: len(ex.word[0]) < 200\n",
    "# )\n",
    "fields=(('word', WORD), ('udtag', UD_TAG), (None, None))\n",
    "train = ConllXDataset('wsj.train0.conllx', fields)\n",
    "val = ConllXDataset('wsj.train0.conllx', fields)\n",
    "\n",
    "#WORD.build_vocab(train.word, min_freq=3)\n",
    "UD_TAG.build_vocab(train.udtag)\n",
    "train_iter = torch_struct.data.TokenBucket(train, 10, device=\"cpu\")\n",
    "val_iter = torchtext.data.BucketIterator(val, \n",
    "    batch_size=10,\n",
    "    device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 10,\n",
       " 'train': True,\n",
       " 'dataset': <__main__.ConllXDataset at 0x104d282d0>,\n",
       " 'batch_size_fn': <function torch_struct.data.data.TokenBucket.<locals>.batch_size_fn(x, _, size)>,\n",
       " 'iterations': 0,\n",
       " 'repeat': True,\n",
       " 'shuffle': True,\n",
       " 'sort': False,\n",
       " 'sort_within_batch': True,\n",
       " 'sort_key': <function torch_struct.data.data.TokenBucket.<locals>.<lambda>(x)>,\n",
       " 'device': 'cpu',\n",
       " 'random_shuffler': <torchtext.data.utils.RandomShuffler at 0x11fa21390>,\n",
       " '_iterations_this_epoch': 0,\n",
       " '_random_state_this_epoch': None,\n",
       " '_restored_from_state': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = len(UD_TAG.vocab)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden, classes):\n",
    "        super().__init__()\n",
    "        self.base_model = model_class.from_pretrained(pretrained_weights)\n",
    "        self.linear = nn.Linear(hidden, C)\n",
    "        self.transition = nn.Linear(C, C)\n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "        \n",
    "    def forward(self, words, mapper):\n",
    "        out = self.dropout(self.base_model(words)[0])\n",
    "        out = torch.einsum(\"bca,bch->bah\", mapper.float(), out) #.cuda()\n",
    "        final = torch.einsum(\"bnh,ch->bnc\", out, self.linear.weight)\n",
    "        batch, N, C = final.shape\n",
    "        vals = final.view(batch, N, C, 1)[:, 1:N] + self.transition.weight.view(1, 1, C, C)\n",
    "        vals[:, 0, :, :] += final.view(batch, N, 1, C)[:, 0] \n",
    "        return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(config[\"H\"], C)\n",
    "\n",
    "x = next(iter(train_iter))\n",
    "words, mapper, _ = x.word\n",
    "label, lengths = x.udtag\n",
    "log_potentials = model(words, mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 22])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 21, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_potentials.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0961,  0.0569,  0.0733,  ...,  0.3130,  0.1020,  0.0248],\n",
       "         [-0.1465, -0.0325, -0.1679,  ..., -0.1838,  0.0979,  0.1399],\n",
       "         [ 0.5245,  0.2689,  0.3400,  ...,  0.4235,  0.2432,  0.3996],\n",
       "         ...,\n",
       "         [-0.4313, -0.3357, -0.3696,  ..., -0.4717, -0.2948, -0.2386],\n",
       "         [ 0.3189,  0.5579,  0.5249,  ...,  0.5096,  0.4506,  0.4154],\n",
       "         [ 0.5427,  0.7105,  0.6433,  ...,  0.6218,  0.6332,  0.7478]]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_potentials[:, 1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(config[\"H\"], C)\n",
    "#wandb.watch(model)\n",
    "#model.cuda()\n",
    "\n",
    "def validate(itera):\n",
    "    incorrect_edges = 0\n",
    "    total = 0 \n",
    "    model.eval()\n",
    "    for i, ex in enumerate(itera):\n",
    "        words, mapper, _ = ex.word\n",
    "        label, lengths = ex.udtag\n",
    "        dist = LinearChainCRF(model(words, mapper), #.cuda()\n",
    "                              lengths=lengths)        \n",
    "        argmax = dist.argmax\n",
    "        gold = LinearChainCRF.struct.to_parts(label.transpose(0, 1), C,\n",
    "                                              lengths=lengths).type_as(argmax)\n",
    "        incorrect_edges += (argmax.sum(-1) - gold.sum(-1)).abs().sum() / 2.0\n",
    "        total += argmax.sum()            \n",
    "        \n",
    "    model.train()    \n",
    "    return incorrect_edges / total   \n",
    "    \n",
    "def train(train_iter, val_iter, model):\n",
    "    opt = AdamW(model.parameters(), lr=1e-4, eps=1e-8)\n",
    "    scheduler = WarmupLinearSchedule(opt, warmup_steps=20, t_total=2500)\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for i, ex in enumerate(train_iter):\n",
    "        opt.zero_grad()\n",
    "        words, mapper, _ = ex.word\n",
    "        label, lengths = ex.udtag\n",
    "        N_1, batch = label.shape\n",
    "\n",
    "        # Model\n",
    "        log_potentials = model(words, mapper) #.cuda()\n",
    "        if not lengths.max() <= log_potentials.shape[1] + 1:\n",
    "            print(\"fail\")\n",
    "            continue\n",
    "\n",
    "        dist = LinearChainCRF(log_potentials,\n",
    "                              lengths=lengths) #lengths.cuda()   \n",
    "\n",
    "        \n",
    "        labels = LinearChainCRF.struct.to_parts(label.transpose(0, 1), C, lengths=lengths) \\\n",
    "                            .type_as(dist.log_potentials)\n",
    "        loss = dist.log_prob(labels).sum()\n",
    "        (-loss).backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        losses.append(loss.detach())\n",
    "        \n",
    "        \n",
    "        if i % 100 == 10:            \n",
    "            print(-torch.tensor(losses).mean(), words.shape)\n",
    "            val_loss = validate(val_iter)\n",
    "            #wandb.log({\"train_loss\":-torch.tensor(losses).mean(), \n",
    "            #           \"val_loss\" : val_loss})\n",
    "            losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(62.9256) torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "train(train_iter, val_iter, model) #.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
