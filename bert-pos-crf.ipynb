{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qqq torchtext wandb pytorch-transformers\n",
    "# !pip install -qqqU git+https://github.com/harvardnlp/pytorch-struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_struct import LinearChainCRF\n",
    "import torch_struct.data\n",
    "import torchtext.data as data\n",
    "from pytorch_transformers import *\n",
    "config = {\"bert\": \"bert-base-cased\", \"H\" : 768, \"dropout\": 0.2}\n",
    "\n",
    "# Comment or add your wandb\n",
    "#import wandb\n",
    "#wandb.init(project=\"pytorch-struct-tagging\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConllXDataset(data.Dataset):\n",
    "    def __init__(self, path, fields, encoding='utf-8', separator='\\t', **kwargs):\n",
    "        examples = []\n",
    "        columns = [[], []]\n",
    "        column_map = {1: 0, 3: 1}\n",
    "        with open(path, encoding=encoding) as input_file:\n",
    "            for line in input_file:\n",
    "                line = line.strip()\n",
    "                if line == '':\n",
    "                    examples.append(data.Example.fromlist(columns, fields))\n",
    "                    columns = [[], []]\n",
    "                else:\n",
    "                    for i, column in enumerate(line.split(separator)):\n",
    "                        if i in column_map:\n",
    "                            columns[column_map[i]].append(column)\n",
    "            examples.append(data.Example.fromlist(columns, fields))\n",
    "        super(ConllXDataset, self).__init__(examples, fields, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = BertModel, BertTokenizer, config[\"bert\"]\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)    \n",
    "WORD = torch_struct.data.SubTokenizedField(tokenizer)\n",
    "UD_TAG = torchtext.data.Field(init_token=\"<bos>\", eos_token=\"<eos>\", include_lengths=True)\n",
    "\n",
    "# train, val, test = torchtext.datasets.UDPOS.splits(\n",
    "#     fields=(('word', WORD), ('udtag', UD_TAG), (None, None)), \n",
    "#     filter_pred=lambda ex: len(ex.word[0]) < 200\n",
    "# )\n",
    "fields=(('word', WORD), ('udtag', UD_TAG), (None, None))\n",
    "train = ConllXDataset('wsj.train0.conllx', fields)\n",
    "val = ConllXDataset('wsj.train0.conllx', fields)\n",
    "\n",
    "#WORD.build_vocab(train.word, min_freq=3)\n",
    "UD_TAG.build_vocab(train.udtag)\n",
    "train_iter = torch_struct.data.TokenBucket(train, 3, device=\"cpu\")\n",
    "val_iter = torchtext.data.BucketIterator(val, \n",
    "    batch_size=10,\n",
    "    device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 3,\n",
       " 'train': True,\n",
       " 'dataset': <__main__.ConllXDataset at 0x105c02550>,\n",
       " 'batch_size_fn': <function torch_struct.data.data.TokenBucket.<locals>.batch_size_fn(x, _, size)>,\n",
       " 'iterations': 0,\n",
       " 'repeat': True,\n",
       " 'shuffle': True,\n",
       " 'sort': False,\n",
       " 'sort_within_batch': True,\n",
       " 'sort_key': <function torch_struct.data.data.TokenBucket.<locals>.<lambda>(x)>,\n",
       " 'device': 'cpu',\n",
       " 'random_shuffler': <torchtext.data.utils.RandomShuffler at 0x1202923d0>,\n",
       " '_iterations_this_epoch': 0,\n",
       " '_random_state_this_epoch': None,\n",
       " '_restored_from_state': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = len(UD_TAG.vocab)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden, classes):\n",
    "        super().__init__()\n",
    "        self.base_model = model_class.from_pretrained(pretrained_weights)\n",
    "        self.linear = nn.Linear(hidden, C)\n",
    "        self.transition = nn.Linear(C, C)\n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "        \n",
    "    def forward(self, words, mapper):\n",
    "        out = self.dropout(self.base_model(words)[0])\n",
    "        out = torch.einsum(\"bca,bch->bah\", mapper.float(), out) #.cuda()\n",
    "        final = torch.einsum(\"bnh,ch->bnc\", out, self.linear.weight)\n",
    "        batch, N, C = final.shape\n",
    "        vals = final.view(batch, N, C, 1)[:, 1:N] + self.transition.weight.view(1, 1, C, C)\n",
    "        vals[:, 0, :, :] += final.view(batch, N, 1, C)[:, 0] \n",
    "        return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 5.2993e-01, -1.3435e-04,  9.2094e-01,  ...,  3.2320e-01,\n",
      "           -8.2655e-03, -1.6876e-01],\n",
      "          [-8.4651e-01, -1.1285e+00, -2.1026e-01,  ..., -8.3672e-01,\n",
      "           -8.6365e-01, -1.4539e+00],\n",
      "          [-6.5025e-02, -3.0551e-01,  3.9340e-01,  ...,  7.1317e-02,\n",
      "           -2.2542e-01, -4.1513e-01],\n",
      "          ...,\n",
      "          [ 2.7483e-02, -2.8510e-01,  4.1860e-01,  ...,  4.2548e-02,\n",
      "           -2.1091e-01, -3.6145e-01],\n",
      "          [ 4.5444e-01, -7.1339e-03,  5.7791e-01,  ...,  3.9465e-01,\n",
      "           -2.3393e-02, -3.1620e-01],\n",
      "          [-8.5504e-01, -9.9617e-01, -3.2160e-01,  ..., -9.7899e-01,\n",
      "           -1.2142e+00, -1.5192e+00]],\n",
      "\n",
      "         [[ 6.1942e-01,  3.9762e-01,  6.6567e-01,  ...,  5.2211e-01,\n",
      "            3.4508e-01,  5.3072e-01],\n",
      "          [-5.9999e-01, -5.7371e-01, -3.0849e-01,  ..., -4.8078e-01,\n",
      "           -3.5328e-01, -5.9742e-01],\n",
      "          [-3.0793e-01, -2.4016e-01, -1.9426e-01,  ..., -6.2171e-02,\n",
      "           -2.0448e-01, -4.8047e-02],\n",
      "          ...,\n",
      "          [-8.7440e-02, -9.1772e-02, -4.1083e-02,  ...,  3.7042e-02,\n",
      "           -6.1986e-02,  1.3361e-01],\n",
      "          [ 4.4788e-01,  2.9456e-01,  2.2659e-01,  ...,  4.9751e-01,\n",
      "            2.3389e-01,  2.8722e-01],\n",
      "          [-7.2337e-01, -5.5625e-01, -5.3470e-01,  ..., -7.3792e-01,\n",
      "           -8.1868e-01, -7.7755e-01]],\n",
      "\n",
      "         [[ 5.8642e-01,  3.6462e-01,  6.3267e-01,  ...,  4.8911e-01,\n",
      "            3.1208e-01,  4.9772e-01],\n",
      "          [-1.3581e-01, -1.0953e-01,  1.5569e-01,  ..., -1.6600e-02,\n",
      "            1.1090e-01, -1.3323e-01],\n",
      "          [-1.6033e-01, -9.2566e-02, -4.6667e-02,  ...,  8.5426e-02,\n",
      "           -5.6879e-02,  9.9550e-02],\n",
      "          ...,\n",
      "          [ 1.3091e-01,  1.2658e-01,  1.7726e-01,  ...,  2.5539e-01,\n",
      "            1.5636e-01,  3.5196e-01],\n",
      "          [ 4.5436e-01,  3.0105e-01,  2.3307e-01,  ...,  5.0399e-01,\n",
      "            2.4038e-01,  2.9371e-01],\n",
      "          [ 1.3219e-02,  1.8034e-01,  2.0190e-01,  ..., -1.3240e-03,\n",
      "           -8.2087e-02, -4.0960e-02]],\n",
      "\n",
      "         [[ 7.0140e-01,  4.7959e-01,  7.4765e-01,  ...,  6.0408e-01,\n",
      "            4.2705e-01,  6.1269e-01],\n",
      "          [-7.4275e-01, -7.1648e-01, -4.5126e-01,  ..., -6.2355e-01,\n",
      "           -4.9605e-01, -7.4018e-01],\n",
      "          [ 3.0168e-01,  3.6945e-01,  4.1535e-01,  ...,  5.4744e-01,\n",
      "            4.0514e-01,  5.6156e-01],\n",
      "          ...,\n",
      "          [-7.5398e-02, -7.9730e-02, -2.9041e-02,  ...,  4.9084e-02,\n",
      "           -4.9944e-02,  1.4565e-01],\n",
      "          [ 1.6357e-01,  1.0253e-02, -5.7719e-02,  ...,  2.1320e-01,\n",
      "           -5.0414e-02,  2.9140e-03],\n",
      "          [-3.7331e-01, -2.0619e-01, -1.8463e-01,  ..., -3.8785e-01,\n",
      "           -4.6862e-01, -4.2749e-01]],\n",
      "\n",
      "         [[ 7.2417e-01,  5.0237e-01,  7.7042e-01,  ...,  6.2686e-01,\n",
      "            4.4983e-01,  6.3547e-01],\n",
      "          [-4.5435e-01, -4.2808e-01, -1.6285e-01,  ..., -3.3515e-01,\n",
      "           -2.0764e-01, -4.5178e-01],\n",
      "          [ 2.2246e-02,  9.0013e-02,  1.3591e-01,  ...,  2.6800e-01,\n",
      "            1.2570e-01,  2.8213e-01],\n",
      "          ...,\n",
      "          [ 2.9857e-01,  2.9424e-01,  3.4493e-01,  ...,  4.2305e-01,\n",
      "            3.2402e-01,  5.1962e-01],\n",
      "          [ 2.6204e-01,  1.0872e-01,  4.0750e-02,  ...,  3.1167e-01,\n",
      "            4.8056e-02,  1.0138e-01],\n",
      "          [-4.3631e-01, -2.6919e-01, -2.4764e-01,  ..., -4.5086e-01,\n",
      "           -5.3162e-01, -4.9049e-01]],\n",
      "\n",
      "         [[ 6.6259e-01,  4.4079e-01,  7.0884e-01,  ...,  5.6528e-01,\n",
      "            3.8825e-01,  5.7389e-01],\n",
      "          [-5.8894e-01, -5.6266e-01, -2.9744e-01,  ..., -4.6973e-01,\n",
      "           -3.4223e-01, -5.8637e-01],\n",
      "          [-1.3144e-01, -6.3675e-02, -1.7777e-02,  ...,  1.1432e-01,\n",
      "           -2.7989e-02,  1.2844e-01],\n",
      "          ...,\n",
      "          [-9.8848e-02, -1.0318e-01, -5.2491e-02,  ...,  2.5633e-02,\n",
      "           -7.3394e-02,  1.2220e-01],\n",
      "          [ 5.4449e-01,  3.9117e-01,  3.2320e-01,  ...,  5.9412e-01,\n",
      "            3.3051e-01,  3.8383e-01],\n",
      "          [-3.7749e-01, -2.1037e-01, -1.8882e-01,  ..., -3.9204e-01,\n",
      "           -4.7280e-01, -4.3167e-01]]]], grad_fn=<CopySlices>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.2993e-01, -1.3435e-04,  9.2094e-01,  ...,  3.2320e-01,\n",
       "           -8.2655e-03, -1.6876e-01],\n",
       "          [-8.4651e-01, -1.1285e+00, -2.1026e-01,  ..., -8.3672e-01,\n",
       "           -8.6365e-01, -1.4539e+00],\n",
       "          [-6.5025e-02, -3.0551e-01,  3.9340e-01,  ...,  7.1317e-02,\n",
       "           -2.2542e-01, -4.1513e-01],\n",
       "          ...,\n",
       "          [ 2.7483e-02, -2.8510e-01,  4.1860e-01,  ...,  4.2548e-02,\n",
       "           -2.1091e-01, -3.6145e-01],\n",
       "          [ 4.5444e-01, -7.1339e-03,  5.7791e-01,  ...,  3.9465e-01,\n",
       "           -2.3393e-02, -3.1620e-01],\n",
       "          [-8.5504e-01, -9.9617e-01, -3.2160e-01,  ..., -9.7899e-01,\n",
       "           -1.2142e+00, -1.5192e+00]],\n",
       "\n",
       "         [[ 6.1942e-01,  3.9762e-01,  6.6567e-01,  ...,  5.2211e-01,\n",
       "            3.4508e-01,  5.3072e-01],\n",
       "          [-5.9999e-01, -5.7371e-01, -3.0849e-01,  ..., -4.8078e-01,\n",
       "           -3.5328e-01, -5.9742e-01],\n",
       "          [-3.0793e-01, -2.4016e-01, -1.9426e-01,  ..., -6.2171e-02,\n",
       "           -2.0448e-01, -4.8047e-02],\n",
       "          ...,\n",
       "          [-8.7440e-02, -9.1772e-02, -4.1083e-02,  ...,  3.7042e-02,\n",
       "           -6.1986e-02,  1.3361e-01],\n",
       "          [ 4.4788e-01,  2.9456e-01,  2.2659e-01,  ...,  4.9751e-01,\n",
       "            2.3389e-01,  2.8722e-01],\n",
       "          [-7.2337e-01, -5.5625e-01, -5.3470e-01,  ..., -7.3792e-01,\n",
       "           -8.1868e-01, -7.7755e-01]],\n",
       "\n",
       "         [[ 5.8642e-01,  3.6462e-01,  6.3267e-01,  ...,  4.8911e-01,\n",
       "            3.1208e-01,  4.9772e-01],\n",
       "          [-1.3581e-01, -1.0953e-01,  1.5569e-01,  ..., -1.6600e-02,\n",
       "            1.1090e-01, -1.3323e-01],\n",
       "          [-1.6033e-01, -9.2566e-02, -4.6667e-02,  ...,  8.5426e-02,\n",
       "           -5.6879e-02,  9.9550e-02],\n",
       "          ...,\n",
       "          [ 1.3091e-01,  1.2658e-01,  1.7726e-01,  ...,  2.5539e-01,\n",
       "            1.5636e-01,  3.5196e-01],\n",
       "          [ 4.5436e-01,  3.0105e-01,  2.3307e-01,  ...,  5.0399e-01,\n",
       "            2.4038e-01,  2.9371e-01],\n",
       "          [ 1.3219e-02,  1.8034e-01,  2.0190e-01,  ..., -1.3240e-03,\n",
       "           -8.2087e-02, -4.0960e-02]],\n",
       "\n",
       "         [[ 7.0140e-01,  4.7959e-01,  7.4765e-01,  ...,  6.0408e-01,\n",
       "            4.2705e-01,  6.1269e-01],\n",
       "          [-7.4275e-01, -7.1648e-01, -4.5126e-01,  ..., -6.2355e-01,\n",
       "           -4.9605e-01, -7.4018e-01],\n",
       "          [ 3.0168e-01,  3.6945e-01,  4.1535e-01,  ...,  5.4744e-01,\n",
       "            4.0514e-01,  5.6156e-01],\n",
       "          ...,\n",
       "          [-7.5398e-02, -7.9730e-02, -2.9041e-02,  ...,  4.9084e-02,\n",
       "           -4.9944e-02,  1.4565e-01],\n",
       "          [ 1.6357e-01,  1.0253e-02, -5.7719e-02,  ...,  2.1320e-01,\n",
       "           -5.0414e-02,  2.9140e-03],\n",
       "          [-3.7331e-01, -2.0619e-01, -1.8463e-01,  ..., -3.8785e-01,\n",
       "           -4.6862e-01, -4.2749e-01]],\n",
       "\n",
       "         [[ 7.2417e-01,  5.0237e-01,  7.7042e-01,  ...,  6.2686e-01,\n",
       "            4.4983e-01,  6.3547e-01],\n",
       "          [-4.5435e-01, -4.2808e-01, -1.6285e-01,  ..., -3.3515e-01,\n",
       "           -2.0764e-01, -4.5178e-01],\n",
       "          [ 2.2246e-02,  9.0013e-02,  1.3591e-01,  ...,  2.6800e-01,\n",
       "            1.2570e-01,  2.8213e-01],\n",
       "          ...,\n",
       "          [ 2.9857e-01,  2.9424e-01,  3.4493e-01,  ...,  4.2305e-01,\n",
       "            3.2402e-01,  5.1962e-01],\n",
       "          [ 2.6204e-01,  1.0872e-01,  4.0750e-02,  ...,  3.1167e-01,\n",
       "            4.8056e-02,  1.0138e-01],\n",
       "          [-4.3631e-01, -2.6919e-01, -2.4764e-01,  ..., -4.5086e-01,\n",
       "           -5.3162e-01, -4.9049e-01]],\n",
       "\n",
       "         [[ 6.6259e-01,  4.4079e-01,  7.0884e-01,  ...,  5.6528e-01,\n",
       "            3.8825e-01,  5.7389e-01],\n",
       "          [-5.8894e-01, -5.6266e-01, -2.9744e-01,  ..., -4.6973e-01,\n",
       "           -3.4223e-01, -5.8637e-01],\n",
       "          [-1.3144e-01, -6.3675e-02, -1.7777e-02,  ...,  1.1432e-01,\n",
       "           -2.7989e-02,  1.2844e-01],\n",
       "          ...,\n",
       "          [-9.8848e-02, -1.0318e-01, -5.2491e-02,  ...,  2.5633e-02,\n",
       "           -7.3394e-02,  1.2220e-01],\n",
       "          [ 5.4449e-01,  3.9117e-01,  3.2320e-01,  ...,  5.9412e-01,\n",
       "            3.3051e-01,  3.8383e-01],\n",
       "          [-3.7749e-01, -2.1037e-01, -1.8882e-01,  ..., -3.9204e-01,\n",
       "           -4.7280e-01, -4.3167e-01]]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(config[\"H\"], C)\n",
    "\n",
    "x = next(iter(train_iter))\n",
    "words, mapper, _ = x.word\n",
    "label, lengths = x.udtag\n",
    "log_potentials = model(words, mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 7])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 32, 32])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_potentials.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6194,  0.3976,  0.6657,  ...,  0.5221,  0.3451,  0.5307],\n",
       "         [-0.6000, -0.5737, -0.3085,  ..., -0.4808, -0.3533, -0.5974],\n",
       "         [-0.3079, -0.2402, -0.1943,  ..., -0.0622, -0.2045, -0.0480],\n",
       "         ...,\n",
       "         [-0.0874, -0.0918, -0.0411,  ...,  0.0370, -0.0620,  0.1336],\n",
       "         [ 0.4479,  0.2946,  0.2266,  ...,  0.4975,  0.2339,  0.2872],\n",
       "         [-0.7234, -0.5562, -0.5347,  ..., -0.7379, -0.8187, -0.7776]]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_potentials[:, 1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(config[\"H\"], C)\n",
    "#wandb.watch(model)\n",
    "#model.cuda()\n",
    "\n",
    "def validate(itera):\n",
    "    incorrect_edges = 0\n",
    "    total = 0 \n",
    "    model.eval()\n",
    "    for i, ex in enumerate(itera):\n",
    "        words, mapper, _ = ex.word\n",
    "        label, lengths = ex.udtag\n",
    "        dist = LinearChainCRF(model(words, mapper), #.cuda()\n",
    "                              lengths=lengths)        \n",
    "        argmax = dist.argmax\n",
    "        gold = LinearChainCRF.struct.to_parts(label.transpose(0, 1), C,\n",
    "                                              lengths=lengths).type_as(argmax)\n",
    "        incorrect_edges += (argmax.sum(-1) - gold.sum(-1)).abs().sum() / 2.0\n",
    "        total += argmax.sum()            \n",
    "        \n",
    "    model.train()    \n",
    "    return incorrect_edges / total   \n",
    "    \n",
    "def train(train_iter, val_iter, model):\n",
    "    opt = AdamW(model.parameters(), lr=1e-4, eps=1e-8)\n",
    "    scheduler = WarmupLinearSchedule(opt, warmup_steps=20, t_total=2500)\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for i, ex in enumerate(train_iter):\n",
    "        opt.zero_grad()\n",
    "        words, mapper, _ = ex.word\n",
    "        label, lengths = ex.udtag\n",
    "        N_1, batch = label.shape\n",
    "\n",
    "        # Model\n",
    "        log_potentials = model(words, mapper) #.cuda()\n",
    "        if not lengths.max() <= log_potentials.shape[1] + 1:\n",
    "            print(\"fail\")\n",
    "            continue\n",
    "\n",
    "        dist = LinearChainCRF(log_potentials,\n",
    "                              lengths=lengths) #lengths.cuda()   \n",
    "\n",
    "        \n",
    "        labels = LinearChainCRF.struct.to_parts(label.transpose(0, 1), C, lengths=lengths) \\\n",
    "                            .type_as(dist.log_potentials)\n",
    "        loss = dist.log_prob(labels).sum()\n",
    "        (-loss).backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        losses.append(loss.detach())\n",
    "        \n",
    "        \n",
    "        if i % 100 == 10:            \n",
    "            print(-torch.tensor(losses).mean(), words.shape)\n",
    "            val_loss = validate(val_iter)\n",
    "            #wandb.log({\"train_loss\":-torch.tensor(losses).mean(), \n",
    "            #           \"val_loss\" : val_loss})\n",
    "            losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1482.2295) torch.Size([21, 35])\n",
      "tensor(527.6224) torch.Size([39, 19])\n",
      "tensor(314.7615) torch.Size([19, 38])\n",
      "tensor(222.4754) torch.Size([17, 44])\n",
      "tensor(176.3658) torch.Size([48, 16])\n",
      "fail\n",
      "tensor(135.1218) torch.Size([46, 16])\n",
      "tensor(140.6739) torch.Size([10, 74])\n",
      "tensor(121.7297) torch.Size([4, 111])\n",
      "tensor(106.9672) torch.Size([17, 44])\n",
      "tensor(88.6975) torch.Size([18, 40])\n",
      "tensor(73.7374) torch.Size([18, 41])\n"
     ]
    }
   ],
   "source": [
    "train(train_iter, val_iter, model) #.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
